{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMvoLEIXq/9Axi4kbwmWxsG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","\n","# Công thức tính CrossEntropy Loss\n","def crossEntropyLoss(output, target):\n","    return F.cross_entropy(output, target)\n","\n","# Công thức tính Mean Square Error\n","def meanSquareError(output, target):\n","    return torch.mean((output - target) ** 2)\n","\n","# Công thức tính BinaryEntropy Loss\n","def binaryEntropyLoss(output, target, n):\n","    loss = -(1/n) * torch.sum(target * torch.log(output) + (1 - target) * torch.log(1 - output))\n","    return loss\n","\n","inputs = torch.tensor([0.1, 0.3, 0.6, 0.7])\n","target = torch.tensor([0.31, 0.32, 0.8, 0.2])\n","n = len(inputs)\n","mse = meanSquareError(inputs, target)\n","binary_loss = binaryEntropyLoss(inputs, target, n)\n","cross_loss = crossEntropyLoss(inputs, target)\n","\n","# In kết quả của các hàm Loss\n","print(f\"Mean Square Error: {mse}\")\n","print(f\"Binary Entropy Loss: {binary_loss}\")\n","print(f\"Cross Entropy Loss: {cross_loss}\")\n","\n","# Công thức hàm sigmoid\n","def sigmoid(x: torch.tensor):\n","    return 1 / (1 + torch.exp(-x))\n","\n","# Công thức hàm relu\n","def relu(x: torch.tensor):\n","    return torch.max(torch.tensor(0.0), x)\n","\n","# Công thức hàm softmax\n","def softmax(zi: torch.tensor):\n","    exp_zi = torch.exp(zi)\n","    return exp_zi / torch.sum(exp_zi)\n","\n","# Công thức hàm tanh\n","def tanh(x: torch.tensor):\n","    return (torch.exp(x) - torch.exp(-x)) / (torch.exp(x) + torch.exp(-x))\n","\n","# Tính toán các hàm activation với một tensor mẫu\n","x = torch.tensor([1.0, 5.0, -4.0, 3.0, -2.0])\n","\n","f_sigmoid = sigmoid(x)\n","f_relu = relu(x)\n","f_softmax = softmax(x)\n","f_tanh = tanh(x)\n","\n","# In kết quả của các hàm activation\n","print(f\"Sigmoid = {f_sigmoid}\")\n","print(f\"Relu = {f_relu}\")\n","print(f\"Softmax = {f_softmax}\")\n","print(f\"Tanh = {f_tanh}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ESERhIThigg2","executionInfo":{"status":"ok","timestamp":1729505301389,"user_tz":-420,"elapsed":1313,"user":{"displayName":"huy vu","userId":"08412247185556521187"}},"outputId":"4f0335a1-d25a-49a0-f2c1-f2ab88311f69"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean Square Error: 0.08362500369548798\n","Binary Entropy Loss: 0.7601855397224426\n","Cross Entropy Loss: 2.2507400512695312\n","Sigmoid = tensor([0.7311, 0.9933, 0.0180, 0.9526, 0.1192])\n","Relu = tensor([1., 5., 0., 3., 0.])\n","Softmax = tensor([1.5862e-02, 8.6604e-01, 1.0688e-04, 1.1721e-01, 7.8972e-04])\n","Tanh = tensor([ 0.7616,  0.9999, -0.9993,  0.9951, -0.9640])\n"]}]}]}